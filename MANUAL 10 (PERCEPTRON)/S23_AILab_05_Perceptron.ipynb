{"cells":[{"cell_type":"markdown","metadata":{"id":"9lgHNnYDZ5LJ"},"source":["# Lab 05 - Perceptron"]},{"cell_type":"markdown","metadata":{"id":"TAuPMPFtZ5LK"},"source":["### Objective\n","\n","The objective of this lesson is to provide an understanding of the Perceptron algorithm, its working principle, and how to implement it from scratch as well as using scikit-learn. By the end of this lesson, you will be able to design and implement a Perceptron model to classify linearly separable data, and evaluate the performance of the model using various metrics.\n","\n","### Learning Outcomes\n","\n","* Understand the working principle of the Perceptron algorithm\n","* Know how to implement the Perceptron algorithm from scratch using Python\n","* Know how to use scikit-learn library to apply the Perceptron algorithm\n","* Understand how to train and evaluate a Perceptron model using various performance metrics\n"]},{"cell_type":"markdown","metadata":{"id":"L6juRACnZ5LM"},"source":["## Introduction to Artificial Neural Network\n"]},{"cell_type":"markdown","metadata":{"id":"_p-InYJFZ5LN"},"source":["Artificial neural networks are inspired by the biological neurons within the human body which activate under certain circumstances resulting in a related action performed by the body in response.\n","\n","An artificial neuron network (neural network) is a computational model that mimics the way nerve cells work in the human brain."]},{"cell_type":"markdown","metadata":{"id":"yipT0_qxZ5LO"},"source":["# Basic Structure of ANNs\n","\n","The human brain is composed of 86 billion nerve cells called neurons. They are connected to other thousand cells by Axons. Stimuli from external environment or inputs from sensory organs are accepted by dendrites. These inputs create electric impulses, which quickly travel through the neural network. A neuron can then send the message to other neuron to handle the issue or does not send it forward."]},{"cell_type":"markdown","metadata":{"id":"kvjkTGueZ5LO"},"source":["![image.png](attachment:image.png)\n","\n","ANNs are composed of multiple nodes, which imitate biological neurons of human brain. The neurons are connected by links and they interact with each other. The nodes can take input data and perform simple operations on the data. The result of these operations is passed to other neurons. The output at each node is called its activation or node value."]},{"cell_type":"markdown","metadata":{"id":"7bSFg9z1Z5LP"},"source":["You can consider an artificial neuron as a mathematical model inspired by a biological neuron.\n","\n","![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"eOVOYuG9Z5LQ"},"source":["* A biological neuron receives its input signals from other neurons through dendrites (small fibers). Likewise, a perceptron receives its data from other perceptrons through input neurons that take numbers.\n","\n","\n","* The connection points between dendrites and biological neurons are called synapses. Likewise, the connections between inputs and perceptrons are called weights. They measure the importance level of each input.\n","\n","\n","* In a biological neuron, the nucleus produces an output signal based on the signals provided by dendrites. Likewise, the nucleus (colored in blue) in a perceptron performs some calculations based on the input values and produces an output.\n","\n","\n","* In a biological neuron, the output signal is carried away by the axon. Likewise, the axon in a perceptron is the output value which will be the input for the next perceptrons."]},{"cell_type":"markdown","metadata":{"id":"2cUrMEROZ5LQ"},"source":["# Perceptron\n","\n","\n","The Perceptron algorithm is a two-class (binary) classification machine learning algorithm. It is a type of neural network model, perhaps the simplest type of neural network model. It consists of a single node or neuron that takes a row of data as input and predicts a class label."]},{"cell_type":"markdown","metadata":{"id":"hVG8OHxOZ5LR"},"source":["# The structure of a perceptron\n","\n","The following image shows a detailed structure of a perceptron. In some contexts, the bias, __b__ is denoted by __w0__. The input, __x0__ always takes the value 1. So, __b*1 = b__."]},{"cell_type":"markdown","metadata":{"id":"LN2V0j_5Z5LR"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"mtl2W6KTZ5LR"},"source":["A perceptron takes the inputs, __x1, x2, …, xn,__ multiplies them by weights, __w1, w2, …, wn__ and adds the bias term, __b__, then computes the linear function, z on which an activation function, __f__ is applied to get the output, __y__."]},{"cell_type":"markdown","metadata":{"id":"Dk8z2ll3Z5LS"},"source":["When drawing a perceptron, we usually ignore the bias unit for our convenience and simplify the diagram as follows. But in calculations, we still consider the bias unit."]},{"cell_type":"markdown","metadata":{"id":"9wG1DTngZ5LT"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"s7Mq2LbpZ5LT"},"source":["# Inside a perceptron\n"]},{"cell_type":"markdown","metadata":{"id":"O9io8bv4Z5LT"},"source":["A perceptron usually consists of two mathematical functions.\n","\n","\n","### Perceptron's linear function\n","\n","This is also called the linear component of the perceptron. It is denoted by z. Its output is the weighted sum of the inputs plus bias unit and can be calculated as follows.\n","\n","\n","![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"PqRVqaCIZ5LT"},"source":["* The x1, x2, …, xn are inputs that take numerical values. There can be several (finite) inputs for a single neuron. They can be raw input data or outputs of the other perceptrons.\n","\n","\n","* The w1, w2, …, wn are weights that take numerical values and control the level of importance of each input. The higher the value, the more important the input.\n","\n","\n","* w1.x1 + w2.x2 + … + wn.xn is called the weighted sum of inputs.\n","\n","\n","* The b is called the bias term or bias unit that also takes a numerical value. It is added to the weighted sum of inputs. The purpose of including a bias term is to shift the activation function of each perceptron to not get a zero value. In other words, if all x1, x2, …, xn inputs are 0, the z is equal to the value of bias.\n"]},{"cell_type":"markdown","metadata":{"id":"mUqbSjy8Z5LT"},"source":["The weights and biases are called the parameters in a neural network model. The optimal values for those parameters are found during the learning (training) process of the neural network."]},{"cell_type":"markdown","metadata":{"id":"w0wF-ZXsZ5LU"},"source":["### Perceptron’s non-linear (activation) function\n","\n","This is also called the non-linear component of the perceptron. It is denoted by f. It is applied on z to get the output y based on the type of activation function we use.\n","\n","![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"rOo8sCuTZ5LU"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"31XwQv8hZ5LU"},"source":["The function f can be a different type of activation function."]},{"cell_type":"markdown","metadata":{"id":"YEkubSskZ5LU"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"LVtD7j8yZ5LU"},"source":["consider the following binary step activation function which is also known as the threshold activation function. We can set any value to the threshold and here we specify the value 0.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OhSYHm2qZ5LU"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"31egXodUZ5LU"},"source":["We say that a neuron or perceptron fires (or activates) only when the value of z exceeds the threshold value, 0. In other words, a neuron outputs 1 (fires or activates) if the value of z exceeds the threshold value, 0. Otherwise, it outputs 0."]},{"cell_type":"markdown","metadata":{"id":"XsVUSNDxZ5LU"},"source":["## Calculations inside a perceptron\n","\n","Let’s perform a simple calculation inside a perception. Imagine that we have 3 inputs with the following values."]},{"cell_type":"markdown","metadata":{"id":"X3SPlSiTZ5LU"},"source":["__x1=2,   x2=3   and   x3=1__\n","\n","Because we have 3 inputs, we also have 3 weights that control the level of importance of each input. Assume the following values for the weights.\n","\n","\n","__w1=0.5, w2=0.2 and w3=10__\n","\n","We also have the following value for the bias unit.\n","\n","__b=2__\n","\n","\n","Let’s calculate the linear function, z.\n","\n","z = (0.5 * 2 + 0.2 * 3 + 10 * 1) + 2\n","\n","z = 13.6"]},{"cell_type":"markdown","metadata":{"id":"VKF0qS0IZ5LV"},"source":["The activation function takes the output of z (13.6) as its input and calculates the output y based on the type of activation function we use. For now, we use the sigmoid activation function defined below.\n","\n","\n","![image.png](attachment:image.png)\n","\n","y = sigmoid(13.6)\n","\n","\n","y = 0.999\n","\n","\n","y ~ 1\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PPbBP1McZ5LV"},"source":["The entire calculation process can be denoted in the following diagram. For ease of understanding, we also denote the bias term in a separate node.\n","\n","![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"EMyL_CXVZ5LV"},"source":["## Stochastic Gradient Descent\n","\n","Gradient Descent is the process of minimizing a function by following the gradients of the cost function.\n","\n","\n","This involves knowing the form of the cost as well as the derivative so that from a given point you know the gradient and can move in that direction, e.g. downhill towards the minimum value.\n","\n","\n","In machine learning, we can use a technique that evaluates and updates the weights every iteration called stochastic gradient descent to minimize the error of a model on our training data.\n","\n","\n","__The way this optimization algorithm works is that each training instance is shown to the model one at a time. The model makes a prediction for a training instance, the error is calculated and the model is updated in order to reduce the error for the next prediction.__\n","\n","\n","This procedure can be used to find the set of weights in a model that result in the smallest error for the model on the training data.\n","\n","\n","For the Perceptron algorithm, each iteration the weights (w) are updated using the equation:\n","\n","\n","__w = w + learning_rate * (expected - predicted) * x__\n","\n","Where w is weight being optimized, learning_rate is a learning rate that you must configure (e.g. 0.01), (expected – predicted) is the prediction error for the model on the training data attributed to the weight and x is the input value."]},{"cell_type":"markdown","metadata":{"id":"_A3XkbkNZ5LW"},"source":["## Algorithm\n","\n","1. Initialize the weights to small random values.\n","\n","\n","2. For each input in the training dataset, calculate the dot product of the input features and the weights, and pass the result through an activation function to get the output prediction.\n","\n","\n","3. Calculate the prediction error as the difference between the predicted output and the true output.\n","\n","\n","4. Update the weights using the prediction error and a learning rate, according to the formula:\n","    __new_weight = old_weight + (learning_rate * error * input_feature)__\n","    \n","    \n","5. Repeat steps 2-4 for all inputs in the training dataset, for a fixed number of epochs or until the weights converge to a stable solution.\n","\n","\n","6. To classify new input data, calculate the dot product of the input features and the learned weights, and pass the result through the activation function to get the final output prediction."]},{"cell_type":"markdown","metadata":{"id":"Y8_t8CP9Z5LW"},"source":["## Making Prediction\n","\n","\n","The first step is to develop a function that can make predictions.\n","\n","\n","\n","This will be needed both in the evaluation of candidate weights values in stochastic gradient descent, and after the model is finalized and we wish to start making predictions on test data or new data.\n","\n","\n","Below is a function named predict() that predicts an output value for a row given a set of weights.\n","\n","\n","The first weight is always the bias as it is standalone and not responsible for a specific input value."]},{"cell_type":"markdown","metadata":{"id":"h9KF8zJuZ5LW"},"source":["### Make a prediction with weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_hhzuN95Z5LW"},"outputs":[],"source":["def predict(row, weights):\n","    activation = weights[0]\n","    for i in range(len(row)-1):\n","        activation += weights[i + 1] * row[i]\n","    return 1.0 if activation >= 0.0 else 0.0"]},{"cell_type":"markdown","metadata":{"id":"7rgQyIEUZ5LX"},"source":["The predict function takes in a row of input data and a set of weights for a perceptron, and returns a prediction for that input data based on the current weight values."]},{"cell_type":"markdown","metadata":{"id":"Jo2Onm2TZ5LX"},"source":["We can contrive a small dataset to test our prediction function.\n","\n","<table>\n","  <tr>\n","    <th>X1</th>\n","    <th>X2</th>\n","    <th>Y</th>\n","  </tr>\n","  <tr>\n","    <td>2.7810836</td>\n","    <td>2.550537003</td>\n","    <td>0</td>\n","  </tr>\n","  <tr>\n","    <td>1.465489372</td>\n","    <td>2.362125076</td>\n","    <td>0</td>\n","  </tr>\n","  <tr>\n","    <td>3.396561688</td>\n","    <td>4.400293529</td>\n","    <td>0</td>\n","  </tr>\n","  <tr>\n","    <td>1.38807019</td>\n","    <td>1.850220317</td>\n","    <td>0</td>\n","  </tr>\n","  <tr>\n","    <td>3.06407232</td>\n","    <td>3.005305973</td>\n","    <td>0</td>\n","  </tr>\n","  <tr>\n","    <td>7.627531214</td>\n","    <td>2.759262235</td>\n","    <td>1</td>\n","  </tr>\n","  <tr>\n","    <td>5.332441248</td>\n","    <td>2.088626775</td>\n","    <td>1</td>\n","  </tr>\n","  <tr>\n","    <td>6.922596716</td>\n","    <td>1.77106367</td>\n","    <td>1</td>\n","  </tr>\n","  <tr>\n","    <td>8.675418651</td>\n","    <td>-0.242068655</td>\n","    <td>1</td>\n","  </tr>\n","  <tr>\n","    <td>7.673756466</td>\n","    <td>3.508563011</td>\n","    <td>1</td>\n","  </tr>\n","</table>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbYMEkPvZ5LX"},"outputs":[],"source":["# test predictions\n","dataset = [[2.7810836,2.550537003,0],\n"," [1.465489372,2.362125076,0],\n"," [3.396561688,4.400293529,0],\n"," [1.38807019,1.850220317,0],\n"," [3.06407232,3.005305973,0],\n"," [7.627531214,2.759262235,1],\n"," [5.332441248,2.088626775,1],\n"," [6.922596716,1.77106367,1],\n"," [8.675418651,-0.242068655,1],\n"," [7.673756466,3.508563011,1]]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wz9cuQigZ5LY","executionInfo":{"status":"ok","timestamp":1745340686692,"user_tz":-300,"elapsed":5,"user":{"displayName":"Muhammad Saood Sarwar","userId":"05753149041264330410"}},"outputId":"9e53a8bb-a184-4143-921c-d4e3bce19c24"},"outputs":[{"output_type":"stream","name":"stdout","text":["Expected=0, Predicted=0\n","Expected=0, Predicted=0\n","Expected=0, Predicted=0\n","Expected=0, Predicted=0\n","Expected=0, Predicted=0\n","Expected=1, Predicted=1\n","Expected=1, Predicted=1\n","Expected=1, Predicted=1\n","Expected=1, Predicted=1\n","Expected=1, Predicted=1\n"]}],"source":["weights = [-0.1, 0.20653640140000007, -0.23418117710000003]\n","for row in dataset:\n","    prediction = predict(row, weights)\n","    print(\"Expected=%d, Predicted=%d\" % (row[-1], prediction))"]},{"cell_type":"markdown","metadata":{"id":"xasIneFbZ5LY"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"C353a3uvZ5Ld"},"source":["Now we are ready to implement stochastic gradient descent to optimize our weight values."]},{"cell_type":"markdown","metadata":{"id":"mMqs7aLjZ5Ld"},"source":["### Training Network Weights\n","\n","We can estimate the weight values for our training data using stochastic gradient descent.\n","\n","Stochastic gradient descent requires two parameters:\n","\n","* __Learning Rate:__ Used to limit the amount each weight is corrected each time it is updated.It is usually set as a small positive number, typically in the range of 0.0 to 1.0. Choosing the optimal learning rate is important as a too small value will lead to a slow convergence and a too high value may cause the algorithm to overshoot the optimal solution. The optimal learning rate depends on the problem and can be found through experimentation and tuning.\n","\n","\n","\n","* __Epochs:__ The number of times to run through the training data while updating the weight. These, along with the training data will be the arguments to the function.\n","\n","\n","There are 3 loops we need to perform in the function:\n","* Loop over each epoch.\n","* Loop over each row in the training data for an epoch.\n","* Loop over each weight and update it for a row in an epoch.\n","\n","As you can see, we update each weight for each row in the training data, each epoch.\n","\n","Weights are updated based on the error the model made. The error is calculated as the difference between the expected output value and the prediction made with the candidate weights. There is one weight for each input attribute, and these are updated in a consistent way, for example:\n","\n","__w(t+1) = w(t) + learning_rate * (expected(t) - predicted(t)) * x(t)__\n","\n","\n","This formula represents how the weights in a linear model are updated during training using the perceptron learning algorithm.\n","\n","* w(t+1) represents the new weight value for a particular feature (x) at time t+1\n","* w(t) represents the current weight value for that feature at time t\n","* learning_rate is a hyperparameter that determines the step size of each update\n","* (expected(t) - predicted(t)) is the error, or the difference between the true label (expected) and the predicted label using the current weights (predicted) for a particular example at time t\n","* x(t) is the value of the input feature for that example at time t"]},{"cell_type":"markdown","metadata":{"id":"dWjz472bZ5Ld"},"source":["The bias is updated in a similar way, except without an input as it is not associated with a specific input value:\n","\n","__bias(t+1) = bias(t) + learning_rate * (expected(t) - predicted(t))__\n","\n","Now we can put all of this together. Below is a function named __train_weights ()__ that calculates weight values for a training dataset using stochastic gradient descent."]},{"cell_type":"markdown","metadata":{"id":"7EVCNDdQZ5Ld"},"source":["### Estimate Perceptron weights using stochastic gradient descent\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kn8JryrsZ5Ld"},"outputs":[],"source":["def train_weights(train, l_rate, n_epoch):\n","\n","    weights = [0.0 for i in range(len(train[0]))]\n","\n","    for epoch in range(n_epoch):\n","\n","        sum_error = 0.0\n","\n","        for row in train:\n","\n","            prediction = predict(row, weights)\n","\n","            error = row[-1] - prediction\n","\n","            sum_error += error**2\n","\n","            weights[0] = weights[0] + l_rate * error  #bias(t+1) = bias(t) + learning_rate * (expected(t) - predicted(t))\n","\n","            for i in range(len(row)-1):\n","\n","                weights[i + 1] = weights[i + 1] + l_rate * error * row[i] #w(t+1) = w(t) + learning_rate * (expected(t) - predicted(t)) * x(t)\n","\n","        print('epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n","\n","    return weights\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0hXlSupwZ5Le","executionInfo":{"status":"ok","timestamp":1745340624345,"user_tz":-300,"elapsed":4,"user":{"displayName":"Muhammad Saood Sarwar","userId":"05753149041264330410"}},"outputId":"467c6897-bb14-4c8f-fb6b-79ecacc890e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch=0, lrate=0.100, error=2.000\n","epoch=1, lrate=0.100, error=1.000\n","epoch=2, lrate=0.100, error=0.000\n","epoch=3, lrate=0.100, error=0.000\n","epoch=4, lrate=0.100, error=0.000\n","[-0.1, 0.20653640140000007, -0.23418117710000003]\n"]}],"source":["l_rate = 0.1\n","n_epoch = 5\n","weights = train_weights(dataset, l_rate, n_epoch)\n","print(weights)"]},{"cell_type":"markdown","metadata":{"id":"CsjlrTYDZ5Le"},"source":["#### Code Explanation\n","\n","Trains the weights of a perceptron model on a given training dataset. Here is a step-by-step explanation of the code:\n","\n","1. __The function takes three arguments:__\n","           train, which is the training dataset,\n","           l_rate, which is the learning rate, and\n","           n_epoch, which is the number of epochs (iterations) to train for.\n","\n","\n","2. A list of initial weights is created with the same length as the number of columns in the training dataset. This is done using a list comprehension.\n","\n","\n","3. A loop is started over the range of n_epoch, which is the number of epochs to train for.\n","\n","\n","4. A variable sum_error is initialized to 0.0. This will be used to keep track of the total error made by the model in each epoch.\n","\n","\n","5. Another loop is started over each row in the training dataset.\n","\n","\n","6. The predict function is called with the current row of data and the current weights of the model to make a prediction.\n","\n","\n","7. The error is calculated as the difference between the expected output value (which is the last column of the row) and the predicted output value.\n","\n","\n","8. The error is squared and added to the sum_error variable.\n","\n","\n","9. The bias weight (which is the first weight in the weights list) is updated by adding the product of the learning rate and the error.\n","\n","\n","10. Another loop is started over each input feature of the row.\n","\n","\n","11. The weight for the current input feature is updated by adding the product of the learning rate, the error, and the input feature value.\n","\n","\n","12. After all rows in the training dataset have been processed, the total error for the epoch is printed to the console.\n","\n","13. After all epochs have been processed, the final weights of the model are returned."]},{"cell_type":"markdown","metadata":{"id":"mW6bbUfyZ5Le"},"source":["You can see how the problem is learned very quickly by the algorithm."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}